{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c816b1cf-9746-433c-a300-03a4e6906a88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client_id     = dbutils.secrets.get(scope=\"kv-olist\", key=\"secret-client-id\")\n",
    "tenant_id     = dbutils.secrets.get(scope=\"kv-olist\", key=\"secret-tenant-id\")\n",
    "client_secret = dbutils.secrets.get(scope=\"kv-olist\", key=\"secret-client-mdp\")\n",
    "\n",
    "storage_account = \"oliststorageaccountbth74\"\n",
    "container_name = \"data\"\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc8645df-0771-44da-812d-c852c4993d9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1) olist_order_reviews_dataset cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9df8c711-77d3-4009-b0b8-9f35007b583a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\", \"pyspark.sql.classic.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"Y2xpZW50X2lkICAgICA9IGRidXRpbHMuc2VjcmV0cy5nZXQoc2NvcGU9Imt2LW9saXN0Iiwga2V5PSJzZWNyZXQtY2xpZW50LWlkIikKdGVuYW50X2lkICAgICA9IGRidXRpbHMuc2VjcmV0cy5nZXQoc2NvcGU9Imt2LW9saXN0Iiwga2V5PSJzZWNyZXQtdGVuYW50LWlkIikKY2xpZW50X3NlY3JldCA9IGRidXRpbHMuc2VjcmV0cy5nZXQoc2NvcGU9Imt2LW9saXN0Iiwga2V5PSJzZWNyZXQtY2xpZW50LW1kcCIpCgpzdG9yYWdlX2FjY291bnQgPSAib2xpc3RzdG9yYWdlYWNjb3VudGJ0aDc0Igpjb250YWluZXJfbmFtZSA9ICJkYXRhIgoKc3BhcmsuY29uZi5zZXQoZiJmcy5henVyZS5hY2NvdW50LmF1dGgudHlwZS57c3RvcmFnZV9hY2NvdW50fS5kZnMuY29yZS53aW5kb3dzLm5ldCIsICJPQXV0aCIpCnNwYXJrLmNvbmYuc2V0KGYiZnMuYXp1cmUuYWNjb3VudC5vYXV0aC5wcm92aWRlci50eXBlLntzdG9yYWdlX2FjY291bnR9LmRmcy5jb3JlLndpbmRvd3MubmV0IiwgIm9yZy5hcGFjaGUuaGFkb29wLmZzLmF6dXJlYmZzLm9hdXRoMi5DbGllbnRDcmVkc1Rva2VuUHJvdmlkZXIiKQpzcGFyay5jb25mLnNldChmImZzLmF6dXJlLmFjY291bnQub2F1dGgyLmNsaWVudC5pZC57c3RvcmFnZV9hY2NvdW50fS5kZnMuY29yZS53aW5kb3dzLm5ldCIsIGNsaWVudF9pZCkKc3BhcmsuY29uZi5zZXQoZiJmcy5henVyZS5hY2NvdW50Lm9hdXRoMi5jbGllbnQuc2VjcmV0LntzdG9yYWdlX2FjY291bnR9LmRmcy5jb3JlLndpbmRvd3MubmV0IiwgY2xpZW50X3NlY3JldCkKc3BhcmsuY29uZi5zZXQoZiJmcy5henVyZS5hY2NvdW50Lm9hdXRoMi5jbGllbnQuZW5kcG9pbnQue3N0b3JhZ2VfYWNjb3VudH0uZGZzLmNvcmUud2luZG93cy5uZXQiLCBmImh0dHBzOi8vbG9naW4ubWljcm9zb2Z0b25saW5lLmNvbS97dGVuYW50X2lkfS9vYXV0aDIvdG9rZW4iKQoKZGYgPSBzcGFyay5yZWFkLmZvcm1hdCgiY3N2IikgXAogICAgLm9wdGlvbigiaGVhZGVyIiwgInRydWUiKSBcCiAgICAub3B0aW9uKCJpbmZlclNjaGVtYSIsICJ0cnVlIikgXAogICAgLm9wdGlvbigibXVsdGlMaW5lIiwgInRydWUiKSBcCiAgICAub3B0aW9uKCJxdW90ZSIsICJcIiIpIFwKICAgIC5vcHRpb24oImVzY2FwZSIsICJcIiIpIFwKICAgIC5sb2FkKGYiYWJmc3M6Ly97Y29udGFpbmVyX25hbWV9QHtzdG9yYWdlX2FjY291bnR9LmRmcy5jb3JlLndpbmRvd3MubmV0L2Jyb256ZS9vbGlzdF9vcmRlcl9yZXZpZXdzX2RhdGFzZXQuY3N2IikKCmRpc3BsYXkoZGYpCgoK\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "Profil de données 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 1766847813460,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": [
        [
         "mimeBundle",
         null
        ]
       ],
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "5b206bd1-d1e9-47c5-8f33-e5361640b80f",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 2.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 1766847807630,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 1766847807593,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .option(\"quote\", \"\\\"\") \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .load(f\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/bronze/olist_order_reviews_dataset.csv\")\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(f\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/silver/olist_order_reviews_dataset\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3e77009-ecb8-4677-82c2-45039d8e0316",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2) olist_products_dataset cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58208b9c-b21e-487d-a6d2-789efd70c096",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\", \"pyspark.sql.classic.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZnJvbSBweXNwYXJrLnNxbC5mdW5jdGlvbnMgaW1wb3J0IGNvbAoKY2xpZW50X2lkICAgICA9IGRidXRpbHMuc2VjcmV0cy5nZXQoc2NvcGU9Imt2LW9saXN0Iiwga2V5PSJzZWNyZXQtY2xpZW50LWlkIikKdGVuYW50X2lkICAgICA9IGRidXRpbHMuc2VjcmV0cy5nZXQoc2NvcGU9Imt2LW9saXN0Iiwga2V5PSJzZWNyZXQtdGVuYW50LWlkIikKY2xpZW50X3NlY3JldCA9IGRidXRpbHMuc2VjcmV0cy5nZXQoc2NvcGU9Imt2LW9saXN0Iiwga2V5PSJzZWNyZXQtY2xpZW50LW1kcCIpCgpzdG9yYWdlX2FjY291bnQgPSAib2xpc3RzdG9yYWdlYWNjb3VudGJ0aDc0Igpjb250YWluZXJfbmFtZSA9ICJkYXRhIgoKc3BhcmsuY29uZi5zZXQoZiJmcy5henVyZS5hY2NvdW50LmF1dGgudHlwZS57c3RvcmFnZV9hY2NvdW50fS5kZnMuY29yZS53aW5kb3dzLm5ldCIsICJPQXV0aCIpCnNwYXJrLmNvbmYuc2V0KGYiZnMuYXp1cmUuYWNjb3VudC5vYXV0aC5wcm92aWRlci50eXBlLntzdG9yYWdlX2FjY291bnR9LmRmcy5jb3JlLndpbmRvd3MubmV0IiwgIm9yZy5hcGFjaGUuaGFkb29wLmZzLmF6dXJlYmZzLm9hdXRoMi5DbGllbnRDcmVkc1Rva2VuUHJvdmlkZXIiKQpzcGFyay5jb25mLnNldChmImZzLmF6dXJlLmFjY291bnQub2F1dGgyLmNsaWVudC5pZC57c3RvcmFnZV9hY2NvdW50fS5kZnMuY29yZS53aW5kb3dzLm5ldCIsIGNsaWVudF9pZCkKc3BhcmsuY29uZi5zZXQoZiJmcy5henVyZS5hY2NvdW50Lm9hdXRoMi5jbGllbnQuc2VjcmV0LntzdG9yYWdlX2FjY291bnR9LmRmcy5jb3JlLndpbmRvd3MubmV0IiwgY2xpZW50X3NlY3JldCkKc3BhcmsuY29uZi5zZXQoZiJmcy5henVyZS5hY2NvdW50Lm9hdXRoMi5jbGllbnQuZW5kcG9pbnQue3N0b3JhZ2VfYWNjb3VudH0uZGZzLmNvcmUud2luZG93cy5uZXQiLCBmImh0dHBzOi8vbG9naW4ubWljcm9zb2Z0b25saW5lLmNvbS97dGVuYW50X2lkfS9vYXV0aDIvdG9rZW4iKQoKZGY9c3BhcmsucmVhZC5mb3JtYXQoImRlbHRhIikubG9hZChmImFiZnNzOi8ve2NvbnRhaW5lcl9uYW1lfUB7c3RvcmFnZV9hY2NvdW50fS5kZnMuY29yZS53aW5kb3dzLm5ldC9zaWx2ZXIvb2xpc3RfcHJvZHVjdHNfZGF0YXNldCIpCgpkZl9maWx0ZXJlZD1kZi5maWxsbmEoewogICAgInByb2R1Y3RfY2F0ZWdvcnlfbmFtZSI6IlVua25vd24iLAogICAgInByb2R1Y3RfbmFtZV9sZW5naHQiOjAsCiAgICAicHJvZHVjdF9kZXNjcmlwdGlvbl9sZW5naHQiOjAsCiAgICAicHJvZHVjdF9waG90b3NfcXR5IjowLAogICAgInByb2R1Y3Rfd2VpZ2h0X2ciOjAsCiAgICAicHJvZHVjdF9sZW5ndGhfY20iOjAsCiAgICAicHJvZHVjdF9oZWlnaHRfY20iOjAsCiAgICAicHJvZHVjdF93aWR0aF9jbSI6MAp9KQoKZGlzcGxheShkZl9maWx0ZXJlZCk=\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "Profil de données 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 1766855189717,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": [
        [
         "mimeBundle",
         null
        ]
       ],
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "307535ba-0542-472a-b688-88f6c04f367d",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 3.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 1766855184951,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 1766855184914,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "df=spark.read.format(\"delta\").load(f\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/silver/olist_products_dataset\")\n",
    "\n",
    "df_filtered=df.fillna({\n",
    "    \"product_category_name\":\"Unknown\",\n",
    "    \"product_name_lenght\":0,\n",
    "    \"product_description_lenght\":0,\n",
    "    \"product_photos_qty\":0,\n",
    "    \"product_weight_g\":0,\n",
    "    \"product_length_cm\":0,\n",
    "    \"product_height_cm\":0,\n",
    "    \"product_width_cm\":0\n",
    "})\n",
    "\n",
    "df_filtered.write.format(\"delta\").mode(\"overwrite\").save(f\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/silver/olist_products_dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b40968a-0f46-45d7-9483-edfa84ede621",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3) olist_order_payments_dataset cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a73aea7c-dcc3-4952-9680-b0ec1ce6a8e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,when\n",
    "\n",
    "\n",
    "df=spark.read.format(\"delta\").load(f\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/silver/olist_order_payments_dataset\")\n",
    "\n",
    "df_filtered=df.filter(col(\"payment_type\")!=\"not_defined\")\n",
    "\n",
    "df_filtered.write.format(\"delta\").mode(\"overwrite\").save(f\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/silver/olist_order_payments_dataset\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8b23f50-014e-418e-b2d8-bb9d046d1cd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4) olist_orders_dataset cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce6cce12-c2e3-4d3b-892a-a3d0bf0baa25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\", \"pyspark.sql.classic.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZnJvbSBweXNwYXJrLnNxbC5mdW5jdGlvbnMgaW1wb3J0IGNvbCx3aGVuCgpjbGllbnRfaWQgICAgID0gZGJ1dGlscy5zZWNyZXRzLmdldChzY29wZT0ia3Ytb2xpc3QiLCBrZXk9InNlY3JldC1jbGllbnQtaWQiKQp0ZW5hbnRfaWQgICAgID0gZGJ1dGlscy5zZWNyZXRzLmdldChzY29wZT0ia3Ytb2xpc3QiLCBrZXk9InNlY3JldC10ZW5hbnQtaWQiKQpjbGllbnRfc2VjcmV0ID0gZGJ1dGlscy5zZWNyZXRzLmdldChzY29wZT0ia3Ytb2xpc3QiLCBrZXk9InNlY3JldC1jbGllbnQtbWRwIikKCnN0b3JhZ2VfYWNjb3VudCA9ICJvbGlzdHN0b3JhZ2VhY2NvdW50YnRoNzQiCmNvbnRhaW5lcl9uYW1lID0gImRhdGEiCgpzcGFyay5jb25mLnNldChmImZzLmF6dXJlLmFjY291bnQuYXV0aC50eXBlLntzdG9yYWdlX2FjY291bnR9LmRmcy5jb3JlLndpbmRvd3MubmV0IiwgIk9BdXRoIikKc3BhcmsuY29uZi5zZXQoZiJmcy5henVyZS5hY2NvdW50Lm9hdXRoLnByb3ZpZGVyLnR5cGUue3N0b3JhZ2VfYWNjb3VudH0uZGZzLmNvcmUud2luZG93cy5uZXQiLCAib3JnLmFwYWNoZS5oYWRvb3AuZnMuYXp1cmViZnMub2F1dGgyLkNsaWVudENyZWRzVG9rZW5Qcm92aWRlciIpCnNwYXJrLmNvbmYuc2V0KGYiZnMuYXp1cmUuYWNjb3VudC5vYXV0aDIuY2xpZW50LmlkLntzdG9yYWdlX2FjY291bnR9LmRmcy5jb3JlLndpbmRvd3MubmV0IiwgY2xpZW50X2lkKQpzcGFyay5jb25mLnNldChmImZzLmF6dXJlLmFjY291bnQub2F1dGgyLmNsaWVudC5zZWNyZXQue3N0b3JhZ2VfYWNjb3VudH0uZGZzLmNvcmUud2luZG93cy5uZXQiLCBjbGllbnRfc2VjcmV0KQpzcGFyay5jb25mLnNldChmImZzLmF6dXJlLmFjY291bnQub2F1dGgyLmNsaWVudC5lbmRwb2ludC57c3RvcmFnZV9hY2NvdW50fS5kZnMuY29yZS53aW5kb3dzLm5ldCIsIGYiaHR0cHM6Ly9sb2dpbi5taWNyb3NvZnRvbmxpbmUuY29tL3t0ZW5hbnRfaWR9L29hdXRoMi90b2tlbiIpCgoKZGY9c3BhcmsucmVhZC5mb3JtYXQoImRlbHRhIikubG9hZChmImFiZnNzOi8ve2NvbnRhaW5lcl9uYW1lfUB7c3RvcmFnZV9hY2NvdW50fS5kZnMuY29yZS53aW5kb3dzLm5ldC9zaWx2ZXIvb2xpc3Rfb3JkZXJzX2RhdGFzZXQiKQoKZGZfZmlsdGVyZWQ9ZGYuZmlsdGVyKGNvbCgib3JkZXJfc3RhdHVzIik9PSJkZWxpdmVyZWQiKQoKZGlzcGxheShkZl9maWx0ZXJlZCk=\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "Profil de données 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 1766858855473,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": [
        [
         "mimeBundle",
         null
        ]
       ],
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "7ea2ac83-ade4-4f04-87af-54e5475e09f6",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 7.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 1766858844804,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 1766858844756,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df=spark.read.format(\"delta\").load(f\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/silver/olist_orders_dataset\")\n",
    "\n",
    "df_filtered=df.filter(col(\"order_status\")==\"delivered\")\n",
    "\n",
    "df_filtered.write.format(\"delta\").mode(\"overwrite\").save(f\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/silver/olist_orders_dataset\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BRONZE_TO_SILVER",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
